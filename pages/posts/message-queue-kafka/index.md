---
draft: false 
title: 消息队列与卡夫卡-阅读文档(on going)
date: 2025-04-24
categories: Middleware
tags:
    - MessageQueue
    - Kafka
    - Java
---

# 开始

## 介绍

### 什么是事件流？
事件流是数字版的人体中枢神经。是从事件源，例如数据库、传感器、移动设备、云服务、应用程序等采集即时数据的一种实现方式。  
并且可以存储这些事件流以便后续检索、操作、处理并即时向事件流反应并可溯源。可以按需求路由事件流去往不同的目标技术。  
事件流保证了数据持续的流动和解释，从而在正确的时间、正确的地点提供正确的信息。

### 事件流可以用来做什么？

很多事

### kafka 是一个事件流平台，这意味着什么？

三点：
- 发布（写）和订阅（读）事件流，包括来自其他系统的持续的输入/输出数据
- 按照你需求的时间内可靠地存储并持续化事件
- 按照事件发生的顺序或者追溯时对事件流进行处理

### 简述 kafka 如何工作

kafka 是一个由复数个**服务端**与**客户端**组成的分布式系统，它们之间通过高性能的 TCP 网络连接通信。可以被部署在裸机、虚拟机和地端或云端的虚拟化容器里。

**服务端**：Kafka 以一个或多个服务器集群的形式运行，可以跨越多个数据中心或云区域。一些服务器构成存储层，称为 *brokers*。其他的一些服务器运行 *Kafka Connect* 来持续性地以事件流形式输入输出数据，来和你现存的系统交互，例如关系型数据库或其他 kafka 集群。为了实现关键性任务用例，kafka 集群拥有高拓展性和容错性，如果任意一台服务器下线，其他服务器将接替其工作以确保不丢失任何关键数据的情况下持续运行。  
**客户端**：通过它们，您可以编写分布式应用程序和微服务，并行、大规模地读取、写入和处理事件流，即使在网络故障或机器故障的情况下也能容错。Kafka 自带了一些这样的客户端，同时还有社区提供的数十种客户端：支持 Java 和 Scala（包括更高级的 Kafka Streams 库）、Go、Python、C/C++ 以及许多其他编程语言的客户端，也支持 REST API。

### 主要概念和术语

一条**事件**记录了你的业务中“某事正在发生”这一现实。当你向 kafka 读/写数据时，会以事件的格式进行。  
An event has a key, value, timestamp, and optional metadata headers.  

一条事件的范例：
- Event key: "Alice"
- Event value: "Made a payment of $200 to Bob"
- Event timestamp: "Jun. 25, 2020 at 2:06 p.m."

*Producers* 是指那些向 kafka 发布（写）事件的客户端，*consumers* 是那些订阅（读并处理）这些事件的。kafka 中，它们相互解耦^[decoupled]并互不相干^[agnostic of each other]。这是实现 kafka 著名的高拓展性的核心设计，例如 producers 从来不需要等待 consumers。kafka 提供了数个保证措施例如单条信息只被处理一次的能力。

事件以 *topics* 的形式被组织和持续化储存。一条 *topic* 就像是一个文件系统中的一个文件夹，事件就是文件夹中的文件。一个典型的 *topic* 名可以是 "payments"。  
一条 *topic* 可以拥有复数个消费者和生产者，一个 *topic* 可以有零个、一个或多个生产者向其写入事件，也可以有零个、一个或多个消费者订阅这些事件。  
*Topic* 中的事件可以按照需求随时阅读，不像传统的消息系统，消息不会“阅后即焚”。而是根据你的需求配置每个 *topic* 来决定每条消息应该存多久。Kafka 的性能基本不受数据大小的影响，因此长时间存储数据完全没有问题。

*Topic* 是被分区的，这意味着一个主题会被分散存储在多个“桶”中，这些桶位于不同的 Kafka broker 上。这种分布式的数据存放方式对可扩展性至关重要，因为它允许客户端应用同时从多个 broker 中读取或写入数据。当一个新事件被发布到某个主题时，实际上是被追加到该主题的某个分区中。具有相同事件键（例如某个客户 ID 或车辆 ID）的事件会写入同一个分区，Kafka 保证任何一个主题分区的消费者总是会按照事件写入的顺序，准确无误地读取这些事件。

![主题的分区](./1-1.png)

为了使数据具备容错性和高可用性，每个主题都可以跨地理区域或数据中心进行复制。无论是系统故障、broker 维护，还是其他问题发生时，总会有多个 broker 拥有数据的副本。生产环境中常见的设置是副本因子为 3，也就是说每条数据始终有三份副本。这种复制是在主题分区（topic-partition）级别进行的。

### Kafka APIs

Kafka 为 Java 和 Scala 提供了五个核心 API：
- Admin API：管理和审查主题、brokers和其他的kafka对象
- Producer API：向一个或多个主题中发布（写）事件流
- Consumer API：订阅（读）并处理一个或多个主题中的事件流
- Kafka Streams API：用于实现流处理应用和微服务。它提供了更高层次的功能来处理事件流，包括转换、带状态的操作（如聚合和连接）、窗口操作、基于事件时间的处理等。输入数据从一个或多个主题中读取，生成的输出则写入一个或多个主题，从而实现对输入流到输出流的有效转换
- Kafka Connect API：构建连接器